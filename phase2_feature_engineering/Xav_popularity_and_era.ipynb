{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tracks_df = pd.read_parquet(\"/Users/xavierhua/Documents/GitHub/bt4222grp9/parquet datasets/tracks_XavierHua.parquet\")\n",
    "tracks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "SPOTIPY_CLIENT_ID = os.getenv(\"SPOTIPY_CLIENT_ID\")\n",
    "SPOTIPY_CLIENT_SECRET = os.getenv(\"SPOTIPY_CLIENT_SECRET\")\n",
    "\n",
    "# Authenticate with Spotify\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\n",
    "    client_id=SPOTIPY_CLIENT_ID,\n",
    "    client_secret=SPOTIPY_CLIENT_SECRET))\n",
    "\n",
    "# Helper: Batch iterator\n",
    "def batch(iterable, n=1):\n",
    "    \"\"\"Yield successive n-sized batches from list.\"\"\"\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "# Load your DataFrame of tracks (if you have it saved before)\n",
    "# tracks_df = pd.read_csv(\"your_tracks_file.csv\")  # Uncomment if needed\n",
    "\n",
    "# Get list of track URIs and album URIs from your DataFrame\n",
    "track_uris = tracks_df[\"track_uri\"].tolist()\n",
    "album_uris = tracks_df[\"album_uri\"].tolist()\n",
    "\n",
    "##############################################\n",
    "# PART 1: Process Tracks with Resume Support #\n",
    "##############################################\n",
    "\n",
    "# Create batches for track URIs (Spotify API supports up to 50 per request)\n",
    "track_batches = list(batch(track_uris, n=50))\n",
    "total_track_batches = len(track_batches)\n",
    "\n",
    "# Check if there's an existing progress file for tracks\n",
    "if os.path.exists(\"track_details_progress.pkl\"):\n",
    "    with open(\"track_details_progress.pkl\", \"rb\") as f:\n",
    "        all_track_details = pickle.load(f)\n",
    "    # Assuming each batch returns 50 tracks (except possibly the last),\n",
    "    # we can estimate how many batches are done.\n",
    "    processed_track_batches = len(all_track_details) // 50\n",
    "    print(f\"Resuming track processing from batch {processed_track_batches + 1} out of {total_track_batches}...\")\n",
    "else:\n",
    "    all_track_details = []\n",
    "    processed_track_batches = 0\n",
    "    print(f\"Starting track processing from batch 1 out of {total_track_batches}...\")\n",
    "\n",
    "# Process remaining track batches\n",
    "for i, track_batch in enumerate(track_batches[processed_track_batches:], start=processed_track_batches + 1):\n",
    "    print(f\"Processing track batch {i}/{total_track_batches}...\")\n",
    "    response = sp.tracks(track_batch)\n",
    "    all_track_details.extend(response[\"tracks\"])\n",
    "    # Save progress every 10 batches (adjust as needed)\n",
    "    if i % 10 == 0:\n",
    "        with open(\"track_details_progress.pkl\", \"wb\") as f:\n",
    "            pickle.dump(all_track_details, f)\n",
    "        print(\"Saved track details progress.\")\n",
    "\n",
    "# Save one more time after completing all batches\n",
    "with open(\"track_details_progress.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_track_details, f)\n",
    "print(\"Track processing completed.\\n\")\n",
    "\n",
    "##############################################\n",
    "# PART 2: Process Albums with Resume Support #\n",
    "##############################################\n",
    "\n",
    "# For album details, we only need unique album URIs.\n",
    "unique_album_uris = list(set(album_uris))\n",
    "album_batches = list(batch(unique_album_uris, n=20))\n",
    "total_album_batches = len(album_batches)\n",
    "\n",
    "# Check if there's an existing progress file for albums\n",
    "if os.path.exists(\"album_details_progress.pkl\"):\n",
    "    with open(\"album_details_progress.pkl\", \"rb\") as f:\n",
    "        album_details_dict = pickle.load(f)\n",
    "    # Estimate number of processed batches.\n",
    "    # (This assumes each complete batch returns 20 albums; the last batch might be smaller.)\n",
    "    processed_album_batches = len(album_details_dict) // 20\n",
    "    print(f\"Resuming album processing from batch {processed_album_batches + 1} out of {total_album_batches}...\")\n",
    "else:\n",
    "    album_details_dict = {}\n",
    "    processed_album_batches = 0\n",
    "    print(f\"Starting album processing from batch 1 out of {total_album_batches}...\")\n",
    "\n",
    "# Process remaining album batches\n",
    "for i, album_batch in enumerate(album_batches[processed_album_batches:], start=processed_album_batches + 1):\n",
    "    print(f\"Processing album batch {i}/{total_album_batches}...\")\n",
    "    response = sp.albums(album_batch)\n",
    "    for album in response[\"albums\"]:\n",
    "        # Skip if album is None or its URI is missing\n",
    "        if album is None or album.get(\"uri\") is None:\n",
    "            continue\n",
    "        album_details_dict[album[\"uri\"]] = album\n",
    "    # Save progress every 5 batches (adjust as needed)\n",
    "    if i % 5 == 0:\n",
    "        with open(\"album_details_progress.pkl\", \"wb\") as f:\n",
    "            pickle.dump(album_details_dict, f)\n",
    "        print(\"Saved album details progress.\")\n",
    "\n",
    "# Save one more time after completing all album batches\n",
    "with open(\"album_details_progress.pkl\", \"wb\") as f:\n",
    "    pickle.dump(album_details_dict, f)\n",
    "print(\"Album processing completed.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 3: Combine Track and Album Metadata #\n",
    "###########################################\n",
    "\n",
    "track_metadata = []\n",
    "for track in all_track_details:\n",
    "    # Skip if track or its album URI is None\n",
    "    if track is None or track[\"album\"].get(\"uri\") is None:\n",
    "        continue\n",
    "    album_uri = track[\"album\"][\"uri\"]\n",
    "    album_info = album_details_dict.get(album_uri, {})\n",
    "    track_metadata.append({\n",
    "        \"track_uri\": track[\"uri\"],\n",
    "        \"track_name\": track[\"name\"],\n",
    "        \"track_popularity\": track[\"popularity\"],\n",
    "        \"album_name\": album_info.get(\"name\", None),\n",
    "        \"album_release_date\": album_info.get(\"release_date\", None),\n",
    "    })\n",
    "\n",
    "# Save the final metadata to CSV\n",
    "metadata_df = pd.DataFrame(track_metadata)\n",
    "metadata_df.to_csv(\"track_metadata.csv\", index=False)\n",
    "\n",
    "print(\"Metadata processing completed and saved to 'track_metadata.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries into a DataFrame\n",
    "track_metadata_df = pd.DataFrame(track_metadata)\n",
    "track_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assume track_metadata_df already has \"track_popularity\" and \"album_release_date\"\n",
    "\n",
    "# # 1. Standardize track popularity\n",
    "# scaler = StandardScaler()\n",
    "# track_metadata_df[\"track_popularity_scaled\"] = scaler.fit_transform(track_metadata_df[[\"track_popularity\"]])\n",
    "\n",
    "# # 2. Define a function to convert release dates to eras (e.g., 70s, 80s, etc.)\n",
    "# def release_date_to_era(release_date):\n",
    "#     try:\n",
    "#         # Convert release_date to datetime; this handles formats like \"YYYY\", \"YYYY-MM-DD\", etc.\n",
    "#         year = pd.to_datetime(release_date, errors='coerce').year\n",
    "#         if pd.isnull(year):\n",
    "#             return None\n",
    "#         if 1970 <= year < 1980:\n",
    "#             return \"70s\"\n",
    "#         elif 1980 <= year < 1990:\n",
    "#             return \"80s\"\n",
    "#         elif 1990 <= year < 2000:\n",
    "#             return \"90s\"\n",
    "#         elif 2000 <= year < 2010:\n",
    "#             return \"00s\"\n",
    "#         elif 2010 <= year < 2020:\n",
    "#             return \"10s\"\n",
    "#         else:\n",
    "#             return \"Other\"\n",
    "#     except Exception as e:\n",
    "#         return None\n",
    "\n",
    "# # Apply the function to create a new column for era\n",
    "# track_metadata_df[\"album_era\"] = track_metadata_df[\"album_release_date\"].apply(release_date_to_era)\n",
    "# track_metadata_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
