{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_idx</th>\n",
       "      <th>track_uri</th>\n",
       "      <th>album_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61588</th>\n",
       "      <td>0</td>\n",
       "      <td>spotify:track:000DfZJww8KiixTKuk9usJ</td>\n",
       "      <td>The Change I'm Seeking</td>\n",
       "      <td>Mike Love</td>\n",
       "      <td>Earthlings</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.513</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-6.376</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.307</td>\n",
       "      <td>120.365</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I just can't take no more\\n I gotta get out of...</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125100</th>\n",
       "      <td>1</td>\n",
       "      <td>spotify:track:000GjfnQc7ggBayDiy1sLW</td>\n",
       "      <td>Y las Mariposas</td>\n",
       "      <td>El Poder De Zacatecas</td>\n",
       "      <td>Abeja Miope</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.748</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-3.274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.864</td>\n",
       "      <td>114.143</td>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208027</th>\n",
       "      <td>2</td>\n",
       "      <td>spotify:track:000JCyEkMFumqCZQJAORiQ</td>\n",
       "      <td>Enough Is Enough</td>\n",
       "      <td>Nipsey Hussle</td>\n",
       "      <td>California Water</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.523</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.483</td>\n",
       "      <td>132.966</td>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102801</th>\n",
       "      <td>3</td>\n",
       "      <td>spotify:track:000VZqvXwT0YNqKk7iG2GS</td>\n",
       "      <td>Dear Youth</td>\n",
       "      <td>The Ghost Inside</td>\n",
       "      <td>Mercy</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.991</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-4.167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.106</td>\n",
       "      <td>124.016</td>\n",
       "      <td>4.0</td>\n",
       "      <td>For whom the bell tolls\\n There's a hurricane ...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69142</th>\n",
       "      <td>4</td>\n",
       "      <td>spotify:track:000uWezkHfg6DbUPf2eDFO</td>\n",
       "      <td>Dancehall Days</td>\n",
       "      <td>The Beautiful Girls</td>\n",
       "      <td>Me I Disconnect From You</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-10.769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.0839</td>\n",
       "      <td>0.360</td>\n",
       "      <td>134.007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46540</th>\n",
       "      <td>252231</td>\n",
       "      <td>spotify:track:7zywhdkPljk4fOyWkh3EqI</td>\n",
       "      <td>Live &amp; Solo At the Artists Den</td>\n",
       "      <td>Ben Kweller</td>\n",
       "      <td>Lizzy</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.269</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-8.274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.403</td>\n",
       "      <td>110.171</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Sign me up I volunteer\\nVotes are in for lifeg...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117614</th>\n",
       "      <td>252232</td>\n",
       "      <td>spotify:track:7zzBEZBTJejWeL6EqWmCD9</td>\n",
       "      <td>All This Bad Blood</td>\n",
       "      <td>Bastille</td>\n",
       "      <td>Get Home</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.525</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-6.745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.186</td>\n",
       "      <td>115.665</td>\n",
       "      <td>4.0</td>\n",
       "      <td>\\nHow am I gonna get myself back home?\\nI, I, ...</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195994</th>\n",
       "      <td>252233</td>\n",
       "      <td>spotify:track:7zzLt6Z9y7jMvXnEg00n58</td>\n",
       "      <td>The Sunny Album (Deluxe Edition)</td>\n",
       "      <td>Hippie Sabotage</td>\n",
       "      <td>Quit Wastin Time</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.581</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-10.225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>0.679</td>\n",
       "      <td>126.910</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No Lyrics</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189208</th>\n",
       "      <td>252234</td>\n",
       "      <td>spotify:track:7zzbfi8fvHe6hm342GcNYl</td>\n",
       "      <td>Ace</td>\n",
       "      <td>Bob Weir</td>\n",
       "      <td>Black-Throated Wind</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.547</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-9.290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.0723</td>\n",
       "      <td>0.669</td>\n",
       "      <td>72.506</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Youre bringing me down, Im running aground,\\n ...</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138481</th>\n",
       "      <td>252235</td>\n",
       "      <td>spotify:track:7zzpwV2lgKsLke68yFoZdp</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Between The Trees</td>\n",
       "      <td>Spain</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.698</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.552</td>\n",
       "      <td>129.996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Can we sail to Spain\\n Just me and you?\\n Open...</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252236 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        track_idx                             track_uri  \\\n",
       "61588           0  spotify:track:000DfZJww8KiixTKuk9usJ   \n",
       "125100          1  spotify:track:000GjfnQc7ggBayDiy1sLW   \n",
       "208027          2  spotify:track:000JCyEkMFumqCZQJAORiQ   \n",
       "102801          3  spotify:track:000VZqvXwT0YNqKk7iG2GS   \n",
       "69142           4  spotify:track:000uWezkHfg6DbUPf2eDFO   \n",
       "...           ...                                   ...   \n",
       "46540      252231  spotify:track:7zywhdkPljk4fOyWkh3EqI   \n",
       "117614     252232  spotify:track:7zzBEZBTJejWeL6EqWmCD9   \n",
       "195994     252233  spotify:track:7zzLt6Z9y7jMvXnEg00n58   \n",
       "189208     252234  spotify:track:7zzbfi8fvHe6hm342GcNYl   \n",
       "138481     252235  spotify:track:7zzpwV2lgKsLke68yFoZdp   \n",
       "\n",
       "                              album_name            artist_name  \\\n",
       "61588             The Change I'm Seeking              Mike Love   \n",
       "125100                   Y las Mariposas  El Poder De Zacatecas   \n",
       "208027                  Enough Is Enough          Nipsey Hussle   \n",
       "102801                        Dear Youth       The Ghost Inside   \n",
       "69142                     Dancehall Days    The Beautiful Girls   \n",
       "...                                  ...                    ...   \n",
       "46540     Live & Solo At the Artists Den            Ben Kweller   \n",
       "117614                All This Bad Blood               Bastille   \n",
       "195994  The Sunny Album (Deluxe Edition)        Hippie Sabotage   \n",
       "189208                               Ace               Bob Weir   \n",
       "138481                             Spain      Between The Trees   \n",
       "\n",
       "                      track_name  danceability  energy  key  loudness  mode  \\\n",
       "61588                 Earthlings         0.631   0.513  2.0    -6.376   1.0   \n",
       "125100               Abeja Miope         0.913   0.748  9.0    -3.274   1.0   \n",
       "208027          California Water         0.795   0.874  0.0    -4.523   1.0   \n",
       "102801                     Mercy         0.444   0.991  7.0    -4.167   1.0   \n",
       "69142   Me I Disconnect From You         0.714   0.635  1.0   -10.769   1.0   \n",
       "...                          ...           ...     ...  ...       ...   ...   \n",
       "46540                      Lizzy         0.575   0.269  7.0    -8.274   1.0   \n",
       "117614                  Get Home         0.599   0.525  9.0    -6.745   0.0   \n",
       "195994          Quit Wastin Time         0.744   0.581  8.0   -10.225   0.0   \n",
       "189208       Black-Throated Wind         0.533   0.547  9.0    -9.290   1.0   \n",
       "138481                     Spain         0.497   0.698  7.0    -2.558   1.0   \n",
       "\n",
       "        speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "61588        0.0293      0.366000          0.000004    0.1090    0.307   \n",
       "125100       0.0428      0.074500          0.000956    0.0403    0.864   \n",
       "208027       0.2100      0.064600          0.000000    0.3410    0.483   \n",
       "102801       0.1330      0.000085          0.000084    0.1200    0.106   \n",
       "69142        0.0299      0.001940          0.259000    0.0839    0.360   \n",
       "...             ...           ...               ...       ...      ...   \n",
       "46540        0.0365      0.779000          0.000000    0.6670    0.403   \n",
       "117614       0.0397      0.729000          0.000046    0.0909    0.186   \n",
       "195994       0.1460      0.593000          0.958000    0.2040    0.679   \n",
       "189208       0.0326      0.029900          0.011300    0.0723    0.669   \n",
       "138481       0.0317      0.127000          0.000000    0.1160    0.552   \n",
       "\n",
       "          tempo  time_signature  \\\n",
       "61588   120.365             4.0   \n",
       "125100  114.143             4.0   \n",
       "208027  132.966             4.0   \n",
       "102801  124.016             4.0   \n",
       "69142   134.007             4.0   \n",
       "...         ...             ...   \n",
       "46540   110.171             4.0   \n",
       "117614  115.665             4.0   \n",
       "195994  126.910             4.0   \n",
       "189208   72.506             4.0   \n",
       "138481  129.996             4.0   \n",
       "\n",
       "                                                   lyrics  duration  \n",
       "61588   I just can't take no more\\n I gotta get out of...       357  \n",
       "125100                                               None       140  \n",
       "208027                                               None       207  \n",
       "102801  For whom the bell tolls\\n There's a hurricane ...       256  \n",
       "69142                                                None       321  \n",
       "...                                                   ...       ...  \n",
       "46540   Sign me up I volunteer\\nVotes are in for lifeg...       233  \n",
       "117614  \\nHow am I gonna get myself back home?\\nI, I, ...       191  \n",
       "195994                                          No Lyrics        69  \n",
       "189208  Youre bringing me down, Im running aground,\\n ...       342  \n",
       "138481  Can we sail to Spain\\n Just me and you?\\n Open...       233  \n",
       "\n",
       "[252236 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(r'../cleaned dataset/track_cleaned.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings:   0%|          | 0/32 [00:00<?, ?batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f5f5c8126b4721b1a5d50af5cf898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 32/1000:   3%|▎         | 1/32 [00:00<00:11,  2.76batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cf128a1e304d9aa4e2b666e420ae59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 64/1000:   6%|▋         | 2/32 [00:00<00:08,  3.38batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f7ccc6b9974037a0591854dfdb1cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 96/1000:   9%|▉         | 3/32 [00:00<00:07,  3.97batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed9b1c919324cf2adabbe76efd39bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 128/1000:  12%|█▎        | 4/32 [00:01<00:06,  4.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6c6f489ea943c39d4a0a8d65391c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 160/1000:  16%|█▌        | 5/32 [00:01<00:05,  4.64batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c19b4063da4197b28766a7e41f84af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 192/1000:  19%|█▉        | 6/32 [00:01<00:05,  4.57batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84db9de8de5b49b8a90009fbaf09dcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 224/1000:  22%|██▏       | 7/32 [00:01<00:05,  4.78batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7bdbc601344e3eabfd513ce1f58b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 256/1000:  25%|██▌       | 8/32 [00:01<00:05,  4.75batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559642f6592c47828ab7780c5c2e1e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 288/1000:  28%|██▊       | 9/32 [00:02<00:04,  4.89batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e0407e9583422996a5f3a6a0de3fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 320/1000:  31%|███▏      | 10/32 [00:02<00:04,  4.83batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0320f56623444148a2c8e6181f17c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 352/1000:  34%|███▍      | 11/32 [00:02<00:04,  4.97batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a81e55ca474afdb6e4d85408353d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 384/1000:  38%|███▊      | 12/32 [00:02<00:04,  4.74batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408c979a126e47ac83417abcf7c20437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 416/1000:  41%|████      | 13/32 [00:02<00:03,  4.77batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b7f73f730c4558a173c9abaddd1ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 448/1000:  44%|████▍     | 14/32 [00:03<00:03,  4.64batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063600c8bcd842d09006ae15d20ba12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 480/1000:  47%|████▋     | 15/32 [00:03<00:03,  4.79batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04cd134dc27446d934fd3ae0a10ba06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 512/1000:  50%|█████     | 16/32 [00:03<00:03,  4.85batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addcdab78eb04f3ba5499c8cf9c60885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 544/1000:  53%|█████▎    | 17/32 [00:03<00:03,  4.97batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f60ac12bb84301892582c9263ec4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 576/1000:  56%|█████▋    | 18/32 [00:03<00:02,  4.95batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5704a0ec0e247e3874cfc7bc704f19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 608/1000:  59%|█████▉    | 19/32 [00:04<00:02,  4.91batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 608\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe363f4bb16e412b89b77dd12e45e182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 640/1000:  62%|██████▎   | 20/32 [00:04<00:02,  5.01batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd82015226c403295c1971995c3d7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 672/1000:  66%|██████▌   | 21/32 [00:04<00:02,  5.12batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0de3f8b45c34f5b894babce35c8dc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 704/1000:  69%|██████▉   | 22/32 [00:04<00:01,  5.05batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ae172c948a48c68d671565272a61c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 736/1000:  72%|███████▏  | 23/32 [00:04<00:01,  5.00batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deee46ecdd5c4c14b464bd008b6aa528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 768/1000:  75%|███████▌  | 24/32 [00:05<00:01,  4.94batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5979a38fd224f3690bff2eec03275e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 800/1000:  78%|███████▊  | 25/32 [00:05<00:01,  5.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224e53b7b98d42039af806c4715bf8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 832/1000:  81%|████████▏ | 26/32 [00:05<00:01,  5.47batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddd8b7ad5d64e07b6c168eec07cb99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 864/1000:  84%|████████▍ | 27/32 [00:05<00:00,  5.79batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45336dfb6beb453fba62a58f78d62a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 896/1000:  88%|████████▊ | 28/32 [00:05<00:00,  6.00batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b5090ca7e94913a4923b2fa1a9ec88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 928/1000:  91%|█████████ | 29/32 [00:05<00:00,  6.21batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3b04d62ee8449e89768e9eeb88d293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 960/1000:  94%|█████████▍| 30/32 [00:06<00:00,  6.24batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa592592179049c698d1e59337d0d99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 992/1000:  97%|█████████▋| 31/32 [00:06<00:00,  6.42batch/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163268abd1e24ad7bd0effc751495dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1024/1000: 100%|██████████| 32/32 [00:06<00:00,  5.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at song 1024\n",
      "✅ Embedding generation complete! Merged with original dataset and saved as 'lyrics_with_embeddings.csv'.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1️⃣ Setup Logging\n",
    "# -------------------------------\n",
    "logging.basicConfig(\n",
    "    filename=\"embedding_generation.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "logging.info(\"Starting embedding generation...\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Load Dataset & Model\n",
    "# -------------------------------\n",
    "df = pd.read_parquet(r'../cleaned dataset/track_cleaned.parquet')\n",
    "\n",
    "# Ensure lyrics column exists\n",
    "if 'lyrics' not in df.columns:\n",
    "    raise ValueError(\"Dataset must contain a 'lyrics' column.\")\n",
    "\n",
    "df = df.dropna(subset=['lyrics'])  # Drop missing lyrics\n",
    "df = df[['track_idx','lyrics']]\n",
    "df = df[:1000]  # Optional: limit data for faster processing\n",
    "\n",
    "# Load multilingual BERT model\n",
    "multi_bert_embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "logging.info(\"Model loaded successfully!\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Resume Progress if Available\n",
    "# -------------------------------\n",
    "save_path = \"lyrics_embeddings_checkpoint.pkl\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    with open(save_path, \"rb\") as f:\n",
    "        saved_data = pickle.load(f)\n",
    "        lyrics_embeddings = saved_data[\"embeddings\"]\n",
    "        last_processed_idx = saved_data[\"last_processed_idx\"]\n",
    "        logging.info(f\"Resuming from song {last_processed_idx}/{len(df)}\")\n",
    "else:\n",
    "    lyrics_embeddings = []\n",
    "    last_processed_idx = 0\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Process Lyrics with Batch Encoding\n",
    "# -------------------------------\n",
    "lyrics_list = df['lyrics'].tolist()\n",
    "total_lyrics = len(lyrics_list)\n",
    "\n",
    "logging.info(f\"Processing {total_lyrics} lyrics for embedding generation.\")\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "# Initialize tqdm progress bar only once\n",
    "progress_bar = tqdm(range(last_processed_idx, total_lyrics, batch_size), desc=\"Generating Embeddings\", unit=\"batch\")\n",
    "\n",
    "for i in progress_bar:\n",
    "    batch_lyrics = lyrics_list[i:i+batch_size]\n",
    "    \n",
    "    batch_embeddings = multi_bert_embedder.encode(batch_lyrics, convert_to_tensor=True).cpu().numpy()\n",
    "    \n",
    "    lyrics_embeddings.extend(batch_embeddings)\n",
    "\n",
    "    # Update tqdm bar text instead of creating a new bar\n",
    "    progress_bar.set_description(f\"Processing {i+batch_size}/{total_lyrics}\")\n",
    "\n",
    "    # Save progress every 100 songs\n",
    "    if (i + batch_size) % 100 < batch_size:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump({\"embeddings\": lyrics_embeddings, \"last_processed_idx\": i + batch_size}, f)\n",
    "        tqdm.write(f\"Checkpoint saved at song {i + batch_size}\")\n",
    "        logging.info(f\"Checkpoint saved at song {i + batch_size}\")\n",
    "\n",
    "logging.info(\"Embedding generation completed!\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ Merge Embeddings with Original Data\n",
    "# -------------------------------\n",
    "df_embeddings = pd.DataFrame(lyrics_embeddings, index=df.index[:len(lyrics_embeddings)])\n",
    "\n",
    "# Merge embeddings back into original dataframe\n",
    "df_final = df.iloc[:len(df_embeddings)].copy()  # Ensure index consistency\n",
    "df_final = pd.concat([df_final, df_embeddings], axis=1)\n",
    "\n",
    "# Save final output\n",
    "df_final.to_csv(\"lyrics_with_embeddings.csv\", index=True)  # Keeping index for tracking\n",
    "\n",
    "logging.info(\"Final embeddings saved successfully!\")\n",
    "os.remove(save_path)  # Delete checkpoint after success\n",
    "\n",
    "print(\"✅ Embedding generation complete! Merged with original dataset and saved as 'lyrics_with_embeddings.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing 999 lyrics in batches of 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Emotions:   0%|          | 0/63 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m batch_lyrics = lyrics_list[i:i+batch_size]\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Run classifier in batch mode\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m results = \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_lyrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43memotions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Extract the most probable emotions and scores\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/pipelines/zero_shot_classification.py:206\u001b[39m, in \u001b[36mZeroShotClassificationPipeline.__call__\u001b[39m\u001b[34m(self, sequences, *args, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1349\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1346\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1347\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1348\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/pipelines/pt_utils.py:269\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1275\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1274\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/pipelines/zero_shot_classification.py:229\u001b[39m, in \u001b[36mZeroShotClassificationPipeline._forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters.keys():\n\u001b[32m    228\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m model_outputs = {\n\u001b[32m    232\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcandidate_label\u001b[39m\u001b[33m\"\u001b[39m: candidate_label,\n\u001b[32m    233\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msequence\u001b[39m\u001b[33m\"\u001b[39m: sequence,\n\u001b[32m    234\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mis_last\u001b[39m\u001b[33m\"\u001b[39m: inputs[\u001b[33m\"\u001b[39m\u001b[33mis_last\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    235\u001b[39m     **outputs,\n\u001b[32m    236\u001b[39m }\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/models/bart/modeling_bart.py:1764\u001b[39m, in \u001b[36mBartForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1760\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1761\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPassing input embeddings is currently not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1762\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1764\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1773\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1780\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# last hidden state\u001b[39;00m\n\u001b[32m   1782\u001b[39m eos_mask = input_ids.eq(\u001b[38;5;28mself\u001b[39m.config.eos_token_id).to(hidden_states.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/models/bart/modeling_bart.py:1527\u001b[39m, in \u001b[36mBartModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1520\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1521\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1522\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1523\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1524\u001b[39m     )\n\u001b[32m   1526\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/models/bart/modeling_bart.py:1379\u001b[39m, in \u001b[36mBartDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1366\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1367\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1368\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m         use_cache,\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1392\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/models/bart/modeling_bart.py:665\u001b[39m, in \u001b[36mBartDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[39m\n\u001b[32m    663\u001b[39m self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    673\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/transformers/models/bart/modeling_bart.py:449\u001b[39m, in \u001b[36mBartSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[39m\n\u001b[32m    446\u001b[39m bsz, tgt_len, _ = hidden_states.size()\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m query_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# get key, value proj\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# `past_key_value[0].shape[2] == key_value_states.shape[1]`\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;66;03m# is checking that the `sequence_length` of the `past_key_value` is the same as\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# the provided `key_value_states` to support prefix tuning\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    455\u001b[39m     is_cross_attention\n\u001b[32m    456\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    457\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m past_key_value[\u001b[32m0\u001b[39m].shape[\u001b[32m2\u001b[39m] == key_value_states.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    458\u001b[39m ):\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m# reuse k,v, cross_attentions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bt4222grp9/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ Load Dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"lyrics_with_embeddings.csv\")  # Ensure it contains the 'lyrics' column\n",
    "\n",
    "if 'lyrics' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'lyrics' column.\")\n",
    "\n",
    "df = df.dropna(subset=['lyrics'])  # Remove missing lyrics\n",
    "lyrics_list = df['lyrics'].tolist()  \n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Load Pre-trained Zero-Shot Classifier (Enable GPU)\n",
    "# -------------------------------\n",
    "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
    "\n",
    "# Define possible emotions\n",
    "emotions = [\n",
    "    \"joy\", \"anger\", \"sadness\", \"fear\", \"love\", \"surprise\", \"disgust\", \"hope\", \"pride\", \"relief\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Perform Zero-Shot Classification (Batch Processing)\n",
    "# -------------------------------\n",
    "batch_size = 16  # Adjust batch size based on available memory\n",
    "predicted_labels = []\n",
    "predicted_scores = []\n",
    "\n",
    "print(f\"🚀 Processing {len(lyrics_list)} lyrics in batches of {batch_size}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(lyrics_list), batch_size), desc=\"Classifying Emotions\"):\n",
    "    batch_lyrics = lyrics_list[i:i+batch_size]\n",
    "    \n",
    "    # Run classifier in batch mode\n",
    "    results = classifier(batch_lyrics, candidate_labels=emotions, multi_label=True)\n",
    "    \n",
    "    # Extract the most probable emotions and scores\n",
    "    for result in results:\n",
    "        predicted_labels.append(result[\"labels\"][0])  # Highest-ranked emotion\n",
    "        predicted_scores.append(result[\"scores\"][0])  # Confidence score\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Save Results to CSV\n",
    "# -------------------------------\n",
    "df_filtered = df.iloc[:len(predicted_labels)].copy()\n",
    "df_filtered[\"predicted_emotion\"] = predicted_labels\n",
    "df_filtered[\"emotion_score\"] = predicted_scores\n",
    "\n",
    "df_filtered[['predicted_emotion','emotion_score']].to_csv(\"lyrics_with_predicted_emotions.csv\", index=False)\n",
    "print(\"✅ Emotion classification complete! Results saved to 'lyrics_with_predicted_emotions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_idx</th>\n",
       "      <th>predicted_emotion</th>\n",
       "      <th>emotion_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>60</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.494299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>72</td>\n",
       "      <td>relief</td>\n",
       "      <td>0.498735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>54</td>\n",
       "      <td>relief</td>\n",
       "      <td>0.498735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>relief</td>\n",
       "      <td>0.498735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>70</td>\n",
       "      <td>relief</td>\n",
       "      <td>0.498735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>relief</td>\n",
       "      <td>0.995897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>98</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.996977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>57</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.997426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>121</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.997516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>140</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.998726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    track_idx predicted_emotion  emotion_score\n",
       "41         60             anger       0.494299\n",
       "49         72            relief       0.498735\n",
       "36         54            relief       0.498735\n",
       "15         22            relief       0.498735\n",
       "48         70            relief       0.498735\n",
       "..        ...               ...            ...\n",
       "34         52            relief       0.995897\n",
       "67         98           sadness       0.996977\n",
       "39         57          surprise       0.997426\n",
       "86        121           sadness       0.997516\n",
       "97        140          surprise       0.998726\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'/Users/xavierhua/Documents/GitHub/bt4222grp9/testing/lyrics_with_predicted_emotions.csv')\n",
    "df = df[['track_idx','predicted_emotion','emotion_score']]\n",
    "df.sort_values('emotion_score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
