{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tracks_df = pd.read_parquet(r\"C:\\Users\\tkeit\\OneDrive\\Documents\\GitHub\\bt4222grp9\\preliminary datasets\\all_songs_with_or_without_lyrics.parquet\")\n",
    "tracks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import spotipy\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from requests.exceptions import ReadTimeout\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "SPOTIPY_CLIENT_ID = os.getenv(\"SPOTIPY_CLIENT_ID\")\n",
    "SPOTIPY_CLIENT_SECRET = os.getenv(\"SPOTIPY_CLIENT_SECRET\")\n",
    "\n",
    "# Authenticate with Spotify\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\n",
    "    client_id=\"c68416b36aeb4eaabca398101322e5b4\",\n",
    "    client_secret=\"44702a064cf740f19ee00de860a7d67a\"\n",
    "))\n",
    "\n",
    "# Helper: Batch iterator\n",
    "def batch(iterable, n=50):\n",
    "    \"\"\"Yield successive n-sized batches from a list.\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]\n",
    "\n",
    "# Load your DataFrame (Ensure track_uri & album_uri exist)\n",
    "#tracks_df = pd.read_csv(\"your_tracks_file.csv\")  # Update with actual file path\n",
    "track_uris = tracks_df[\"track_uri\"].tolist()\n",
    "album_uris = tracks_df[\"album_name\"].tolist()\n",
    "\n",
    "##############################################\n",
    "# PART 1: Process Tracks with Resume Support #\n",
    "##############################################\n",
    "\n",
    "# Define the progress file\n",
    "TRACK_PROGRESS_FILE = \"track_details_progressAll.pkl\"\n",
    "PROCESSED_TRACKS_FILE = \"processed_tracksALL.pkl\"\n",
    "\n",
    "# Load existing progress if available\n",
    "if os.path.exists(TRACK_PROGRESS_FILE) and os.path.getsize(TRACK_PROGRESS_FILE) > 0:\n",
    "    try:\n",
    "        with open(TRACK_PROGRESS_FILE, \"rb\") as f:\n",
    "            all_track_details = pickle.load(f)\n",
    "            processed_track_uris = {track[\"id\"] for track in all_track_details if track and isinstance(track, dict) and \"id\" in track}\n",
    "            print(f\"Resuming track processing... {len(processed_track_uris)} tracks already processed.\")\n",
    "    except (EOFError, pickle.UnpicklingError):\n",
    "        print(\"Error: Pickle file is empty or corrupted. Starting fresh...\")\n",
    "        all_track_details = []\n",
    "        processed_track_uris = set()\n",
    "else:\n",
    "    print(\"No valid progress file found. Starting track processing from scratch...\")\n",
    "    all_track_details = []\n",
    "    processed_track_uris = set()\n",
    "\n",
    "# Function to fetch track details with error handling\n",
    "def fetch_track_details(track_batch):\n",
    "    try:\n",
    "        response = sp.tracks(track_batch)\n",
    "        return response[\"tracks\"]\n",
    "    except ReadTimeout:\n",
    "        print(\"Timeout error, retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        return fetch_track_details(track_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching tracks: {e}\")\n",
    "        return []\n",
    "\n",
    "# Process track batches efficiently using ThreadPoolExecutor\n",
    "track_batches = list(batch(track_uris, 50))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for i, track_batch in enumerate(track_batches):\n",
    "        if set(track_batch).issubset(processed_track_uris):\n",
    "            continue  # Skip already processed tracks\n",
    "        \n",
    "        print(f\"Processing batch {i+1}/{len(track_batches)}...\")\n",
    "        track_results = executor.submit(fetch_track_details, track_batch).result()\n",
    "        \n",
    "        if track_results:\n",
    "            all_track_details.extend(track_results)\n",
    "            processed_track_uris.update(track_batch)\n",
    "\n",
    "        # Save progress every 10 batches\n",
    "        if (i + 1) % 10 == 0:\n",
    "            with open(TRACK_PROGRESS_FILE, \"wb\") as f:\n",
    "                pickle.dump(all_track_details, f)\n",
    "            print(\"Track details progress saved.\")\n",
    "\n",
    "# Final save after completion\n",
    "with open(TRACK_PROGRESS_FILE, \"wb\") as f:\n",
    "    pickle.dump(all_track_details, f)\n",
    "\n",
    "print(\"Track processing completed.\")\n",
    "\n",
    "##############################################\n",
    "# PART 2: Process Albums with Resume Support #\n",
    "##############################################\n",
    "\n",
    "ALBUM_PROGRESS_FILE = \"album_details_progressALL.pkl\"\n",
    "PROCESSED_ALBUMS_FILE = \"processed_albumsALL.pkl\"\n",
    "\n",
    "# Load existing album progress if available\n",
    "if os.path.exists(ALBUM_PROGRESS_FILE):\n",
    "    with open(ALBUM_PROGRESS_FILE, \"rb\") as f:\n",
    "        album_details_dict = pickle.load(f)\n",
    "    processed_album_uris = set(album_details_dict.keys())\n",
    "    print(f\"Resuming album processing... {len(processed_album_uris)} albums already processed.\")\n",
    "else:\n",
    "    album_details_dict = {}\n",
    "    processed_album_uris = set()\n",
    "    print(\"Starting album processing from scratch...\")\n",
    "\n",
    "# Function to fetch album details with error handling\n",
    "def fetch_album_details(album_batch):\n",
    "    try:\n",
    "        response = sp.albums(album_batch)\n",
    "        return {album[\"uri\"]: album for album in response[\"albums\"] if album}\n",
    "    except ReadTimeout:\n",
    "        print(\"Timeout error, retrying in 5 seconds...\")\n",
    "        time.sleep(5)\n",
    "        return fetch_album_details(album_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching albums: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Process album batches efficiently using ThreadPoolExecutor\n",
    "unique_album_uris = list(set(album_uris))\n",
    "album_batches = list(batch(unique_album_uris, 20))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for i, album_batch in enumerate(album_batches):\n",
    "        if set(album_batch).issubset(processed_album_uris):\n",
    "            continue  # Skip already processed albums\n",
    "        \n",
    "        print(f\"Processing album batch {i+1}/{len(album_batches)}...\")\n",
    "        album_results = executor.submit(fetch_album_details, album_batch).result()\n",
    "        \n",
    "        if album_results:\n",
    "            album_details_dict.update(album_results)\n",
    "            processed_album_uris.update(album_batch)\n",
    "\n",
    "        # Save progress every 5 batches\n",
    "        if (i + 1) % 5 == 0:\n",
    "            with open(ALBUM_PROGRESS_FILE, \"wb\") as f:\n",
    "                pickle.dump(album_details_dict, f)\n",
    "            print(\"Album details progress saved.\")\n",
    "\n",
    "# Final save after completion\n",
    "with open(ALBUM_PROGRESS_FILE, \"wb\") as f:\n",
    "    pickle.dump(album_details_dict, f)\n",
    "\n",
    "print(\"Album processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# PART 3: Combine Track and Album Metadata #\n",
    "###########################################\n",
    "\n",
    "# Load track details\n",
    "TRACK_PROGRESS_FILE = \"track_details_progress.pkl\"\n",
    "ALBUM_PROGRESS_FILE = \"album_details_progress.pkl\"\n",
    "\n",
    "# Load saved track data\n",
    "if os.path.exists(TRACK_PROGRESS_FILE):\n",
    "    with open(TRACK_PROGRESS_FILE, \"rb\") as f:\n",
    "        all_track_details = pickle.load(f)\n",
    "    # Remove any None or corrupted data\n",
    "    all_track_details = [track for track in all_track_details if track and isinstance(track, dict)]\n",
    "else:\n",
    "    raise FileNotFoundError(\"Track data file not found. Ensure track processing has been completed.\")\n",
    "\n",
    "# Load saved album data\n",
    "if os.path.exists(ALBUM_PROGRESS_FILE):\n",
    "    with open(ALBUM_PROGRESS_FILE, \"rb\") as f:\n",
    "        album_details_dict = pickle.load(f)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Album data file not found. Ensure album processing has been completed.\")\n",
    "\n",
    "# Function to process and merge track & album metadata\n",
    "def process_track_metadata(track):\n",
    "    \"\"\"Processes individual track metadata by merging album details.\"\"\"\n",
    "    try:\n",
    "        if not track or \"album\" not in track or track[\"album\"].get(\"uri\") is None:\n",
    "            return None  # Skip if no album data\n",
    "        \n",
    "        album_uri = track[\"album\"][\"uri\"]\n",
    "        album_info = album_details_dict.get(album_uri, {})\n",
    "\n",
    "        return {\n",
    "            \"track_uri\": track.get(\"id\"),  # Ensure correct Spotify ID format\n",
    "            \"track_name\": track.get(\"name\"),\n",
    "            \"track_popularity\": track.get(\"popularity\"),\n",
    "            \"artist_name\": track[\"artists\"][0][\"name\"] if \"artists\" in track and track[\"artists\"] else None,\n",
    "            \"album_name\": album_info.get(\"name\", None),\n",
    "            \"album_release_date\": album_info.get(\"release_date\", None),\n",
    "            \"release_date_precision\": album_info.get(\"release_date_precision\", None)  # Ensuring track release precision\n",
    "        \n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing track {track.get('id', 'Unknown')}: {e}\")\n",
    "        return None  # Avoid crashes on errors\n",
    "\n",
    "# Use ThreadPoolExecutor to speed up metadata processing\n",
    "track_metadata = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(executor.map(process_track_metadata, all_track_details))\n",
    "    track_metadata = [r for r in results if r is not None]  # Remove None values\n",
    "\n",
    "# Convert list to DataFrame\n",
    "metadata_df = pd.DataFrame(track_metadata)\n",
    "\n",
    "# Save final metadata to CSV\n",
    "metadata_df.to_csv(\"track_metadata2.csv\", index=False)\n",
    "\n",
    "print(f\"Metadata processing completed and saved to 'track_metadata2.csv'. Total records: {len(metadata_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# ✅ Load environment variables\n",
    "load_dotenv()\n",
    "SPOTIPY_CLIENT_ID = os.getenv(\"SPOTIPY_CLIENT_ID\")\n",
    "SPOTIPY_CLIENT_SECRET = os.getenv(\"SPOTIPY_CLIENT_SECRET\")\n",
    "\n",
    "# ✅ Authenticate with Spotify API\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\n",
    "    client_id=\"e7815f861b194762a562b79ecfd55ceb\",\n",
    "    client_secret=\"8e01abeb62a1439f84cab00276dfc37a\"\n",
    "))\n",
    "\n",
    "# ✅ Load your existing dataset (which already contains track_uri, track_name, album_name, artist_name)\n",
    "df = pd.read_parquet(r\"C:\\Users\\tkeit\\OneDrive\\Documents\\GitHub\\bt4222grp9\\preliminary datasets\\all_songs_with_or_without_lyrics.parquet\")\n",
    "\n",
    "# ✅ Extract unique Track URIs, Album Names, and Artist Names\n",
    "track_uris = df[\"track_uri\"].dropna().unique().tolist()\n",
    "album_names = df[\"album_name\"].dropna().unique().tolist()\n",
    "artist_names = df[\"artist_name\"].dropna().unique().tolist()\n",
    "\n",
    "# ✅ Paths for caching progress\n",
    "TRACK_PROGRESS_FILE = \"track_popularity_progress.pkl\"\n",
    "ALBUM_PROGRESS_FILE = \"album_release_progress.pkl\"\n",
    "ARTIST_PROGRESS_FILE = \"artist_genre_progress.pkl\"\n",
    "\n",
    "# ✅ Load previous progress to prevent duplicate API calls\n",
    "def load_progress(file_path):\n",
    "    \"\"\"Loads saved progress to avoid redundant API calls.\"\"\"\n",
    "    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return {}\n",
    "\n",
    "# ✅ Load previously processed data\n",
    "track_popularity_dict = load_progress(TRACK_PROGRESS_FILE)\n",
    "album_release_dict = load_progress(ALBUM_PROGRESS_FILE)\n",
    "artist_genre_dict = load_progress(ARTIST_PROGRESS_FILE)\n",
    "\n",
    "# ✅ Fetch track popularity\n",
    "def fetch_track_popularity(track_batch):\n",
    "    \"\"\"Fetches track popularity scores from Spotify.\"\"\"\n",
    "    try:\n",
    "        response = sp.tracks(track_batch)\n",
    "        return {track[\"id\"]: track[\"popularity\"] for track in response[\"tracks\"] if track}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching track popularity: {e}\")\n",
    "        return {}\n",
    "\n",
    "# ✅ Fetch album release date\n",
    "def fetch_album_release(album_batch):\n",
    "    \"\"\"Fetches album release dates from Spotify.\"\"\"\n",
    "    try:\n",
    "        response = sp.search(q=f\"album:{album_batch}\", type=\"album\", limit=1)\n",
    "        if response[\"albums\"][\"items\"]:\n",
    "            album = response[\"albums\"][\"items\"][0]\n",
    "            return {album[\"name\"]: album[\"release_date\"]}\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching album release dates: {e}\")\n",
    "        return {}\n",
    "\n",
    "# ✅ Fetch artist genres\n",
    "def fetch_artist_genre(artist_batch):\n",
    "    \"\"\"Fetches genres for a list of artists from Spotify.\"\"\"\n",
    "    try:\n",
    "        response = sp.search(q=f\"artist:{artist_batch}\", type=\"artist\", limit=1)\n",
    "        if response[\"artists\"][\"items\"]:\n",
    "            artist = response[\"artists\"][\"items\"][0]\n",
    "            return {artist[\"name\"]: \", \".join(artist[\"genres\"])}  # Convert list to string\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching artist genres: {e}\")\n",
    "        return {}\n",
    "\n",
    "# ✅ Process track popularity in batches\n",
    "track_batches = [track_uris[i:i+50] for i in range(0, len(track_uris), 50)]\n",
    "for batch in tqdm(track_batches, desc=\"Fetching Track Popularity\"):\n",
    "    if set(batch).issubset(track_popularity_dict.keys()):\n",
    "        continue  # Skip already processed tracks\n",
    "    track_popularity_dict.update(fetch_track_popularity(batch))\n",
    "\n",
    "    # Save progress every 500 entries\n",
    "    if len(track_popularity_dict) % 500 == 0:\n",
    "        with open(TRACK_PROGRESS_FILE, \"wb\") as f:\n",
    "            pickle.dump(track_popularity_dict, f)\n",
    "\n",
    "# ✅ Process album release dates in batches\n",
    "album_batches = [album_names[i:i+20] for i in range(0, len(album_names), 20)]\n",
    "for batch in tqdm(album_batches, desc=\"Fetching Album Release Dates\"):\n",
    "    if set(batch).issubset(album_release_dict.keys()):\n",
    "        continue  # Skip already processed albums\n",
    "    album_release_dict.update(fetch_album_release(batch))\n",
    "\n",
    "    # Save progress every 100 entries\n",
    "    if len(album_release_dict) % 100 == 0:\n",
    "        with open(ALBUM_PROGRESS_FILE, \"wb\") as f:\n",
    "            pickle.dump(album_release_dict, f)\n",
    "\n",
    "# ✅ Process artist genres in batches\n",
    "artist_batches = [artist_names[i:i+10] for i in range(0, len(artist_names), 10)]\n",
    "for batch in tqdm(artist_batches, desc=\"Fetching Artist Genres\"):\n",
    "    if set(batch).issubset(artist_genre_dict.keys()):\n",
    "        continue  # Skip already processed artists\n",
    "    artist_genre_dict.update(fetch_artist_genre(batch))\n",
    "\n",
    "    # Save progress every 100 entries\n",
    "    if len(artist_genre_dict) % 100 == 0:\n",
    "        with open(ARTIST_PROGRESS_FILE, \"wb\") as f:\n",
    "            pickle.dump(artist_genre_dict, f)\n",
    "\n",
    "# ✅ Merge newly fetched data with existing dataset\n",
    "df[\"track_popularity\"] = df[\"track_uri\"].map(track_popularity_dict)\n",
    "df[\"album_release_date\"] = df[\"album_name\"].map(album_release_dict)\n",
    "df[\"artist_genre\"] = df[\"artist_name\"].map(artist_genre_dict)\n",
    "\n",
    "# ✅ Save updated dataset\n",
    "df.to_parquet(\"your_dataset_with_popularity_and_genre.parquet\", index=False)\n",
    "print(\"✅ Updated dataset saved as 'your_dataset_with_popularity_and_genre.parquet'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify output and Load the newly created file\n",
    "df_new = pd.read_csv(\"track_metadata2.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "print(df_new.head())\n",
    "\n",
    "# Check total records\n",
    "print(f\"Total records in new file: {len(df_new)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure track_metadata is not empty\n",
    "if not track_metadata:\n",
    "    raise ValueError(\"Error: track_metadata list is empty. Check the track processing step.\")\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "track_metadata_df = pd.DataFrame(track_metadata)\n",
    "\n",
    "# Check if DataFrame is successfully created\n",
    "if track_metadata_df.empty:\n",
    "    raise ValueError(\"Error: track_metadata_df is empty after conversion. Verify data extraction.\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"Successfully converted track metadata to DataFrame:\")\n",
    "print(track_metadata_df.head())\n",
    "\n",
    "# Optional: Save DataFrame to CSV for verification\n",
    "track_metadata_df.to_csv(\"track_metadata_df.csv\", index=False)\n",
    "print(\"Track metadata DataFrame saved as 'track_metadata_df.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "######################################\n",
    "# PART 1: Standardizing Track Popularity #\n",
    "######################################\n",
    "\n",
    "# Ensure 'track_popularity' exists before scaling\n",
    "if \"track_popularity\" in track_metadata_df.columns:\n",
    "    scaler = StandardScaler()\n",
    "    track_metadata_df[\"track_popularity_scaled\"] = scaler.fit_transform(track_metadata_df[[\"track_popularity\"]])\n",
    "else:\n",
    "    raise KeyError(\"Error: 'track_popularity' column missing from DataFrame.\")\n",
    "\n",
    "######################################\n",
    "# PART 2: Categorizing Release Date into Eras #\n",
    "######################################\n",
    "\n",
    "def release_date_to_era(release_date):\n",
    "    \"\"\"\n",
    "    Converts a song's release date into a categorized music era.\n",
    "\n",
    "    Arguments:\n",
    "    - release_date (str): The release date from Spotify API. Format: YYYY-MM-DD, YYYY-MM, or YYYY\n",
    "\n",
    "    Returns:\n",
    "    - str: The corresponding music era, or None if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert release_date to standardized year format\n",
    "        year = pd.to_datetime(release_date, errors=\"coerce\").year\n",
    "\n",
    "        # Handle missing or invalid dates\n",
    "        if pd.isnull(year):\n",
    "            return None\n",
    "\n",
    "        # Categorize into refined music eras\n",
    "        if year < 1950:\n",
    "            return \"Pre-1950s\"\n",
    "        elif 1950 <= year < 1960:\n",
    "            return \"1950s Rock & Roll\"\n",
    "        elif 1960 <= year < 1970:\n",
    "            return \"1960s Classic Rock & Motown\"\n",
    "        elif 1970 <= year < 1980:\n",
    "            return \"1970s Disco & Hard Rock\"\n",
    "        elif 1980 <= year < 1990:\n",
    "            return \"1980s Pop & New Wave\"\n",
    "        elif 1990 <= year < 2000:\n",
    "            return \"1990s Alternative & Hip-Hop Boom\"\n",
    "        elif 2000 <= year < 2010:\n",
    "            return \"2000s Digital Pop & Emo\"\n",
    "        elif 2010 <= year < 2020:\n",
    "            return \"2010s Streaming & Trap\"\n",
    "        elif 2020 <= year < 2030:\n",
    "            return \"2020s Present Era\"\n",
    "        else:\n",
    "            return \"Future Music\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None  # Catch unexpected errors\n",
    "\n",
    "# Apply function to categorize all release dates\n",
    "if \"album_release_date\" in track_metadata_df.columns:\n",
    "    track_metadata_df[\"album_era\"] = track_metadata_df[\"album_release_date\"].apply(release_date_to_era)\n",
    "else:\n",
    "    raise KeyError(\"Error: 'album_release_date' column missing from DataFrame.\")\n",
    "\n",
    "# Display first few rows for verification (optional)\n",
    "print(track_metadata_df[[\"album_release_date\", \"album_era\"]].head())\n",
    "\n",
    "# Save the updated DataFrame to a new file for analysis\n",
    "track_metadata_df.to_csv(\"track_metadata_with_eras.csv\", index=False)\n",
    "print(\"Updated track metadata saved to 'track_metadata_with_eras.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure 'track_popularity' column exists before visualization\n",
    "if \"track_popularity\" not in track_metadata_df.columns:\n",
    "    raise KeyError(\"Error: 'track_popularity' column missing from dataset.\")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create a violin + boxplot hybrid for deeper analysis\n",
    "sns.violinplot(\n",
    "    x=\"album_era\", \n",
    "    y=\"track_popularity\", \n",
    "    data=track_metadata_df, \n",
    "    inner=\"quartile\", \n",
    "    palette=\"coolwarm\"\n",
    ")\n",
    "\n",
    "sns.boxplot(\n",
    "    x=\"album_era\", \n",
    "    y=\"track_popularity\", \n",
    "    data=track_metadata_df, \n",
    "    width=0.3, \n",
    "    boxprops={'facecolor':'None'}, \n",
    "    showcaps=False\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Music Era\", fontsize=12)\n",
    "plt.ylabel(\"Popularity Score\", fontsize=12)\n",
    "plt.title(\"🔥 Popularity Distribution Across Music Eras\", fontsize=14)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'track_popularity' column exists\n",
    "if \"track_popularity\" not in track_metadata_df.columns:\n",
    "    raise KeyError(\"Error: 'track_popularity' column missing from dataset.\")\n",
    "\n",
    "# Sort the DataFrame by era for better visualization\n",
    "track_metadata_df = track_metadata_df.sort_values(\"album_era\")\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the ridgeline plot (density stacked by era)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.kdeplot(\n",
    "    data=track_metadata_df, \n",
    "    x=\"track_popularity\", \n",
    "    hue=\"album_era\", \n",
    "    fill=True, \n",
    "    palette=\"coolwarm\", \n",
    "    alpha=0.7, \n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Popularity Score\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.title(\"Popularity Evolution Over Time - Ridgeline Density Plot\", fontsize=14)\n",
    "plt.legend(title=\"Music Era\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# If 'retrieval_date' is missing, create it using today's date\n",
    "if \"retrieval_date\" not in track_metadata_df.columns:\n",
    "    track_metadata_df[\"retrieval_date\"] = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Convert to datetime format\n",
    "track_metadata_df[\"retrieval_date\"] = pd.to_datetime(track_metadata_df[\"retrieval_date\"])\n",
    "\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = [\"album_era\", \"track_popularity\", \"retrieval_date\"]\n",
    "for col in required_columns:\n",
    "    if col not in track_metadata_df.columns:\n",
    "        raise KeyError(f\"Error: '{col}' column missing from dataset.\")\n",
    "\n",
    "# Convert retrieval_date to datetime for accurate plotting\n",
    "track_metadata_df[\"retrieval_date\"] = pd.to_datetime(track_metadata_df[\"retrieval_date\"])\n",
    "\n",
    "# Group data by era and retrieval year, then compute average popularity\n",
    "popularity_over_time = (\n",
    "    track_metadata_df.groupby([track_metadata_df[\"retrieval_date\"].dt.year, \"album_era\"])[\"track_popularity\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"retrieval_date\": \"year\"})\n",
    ")\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot popularity evolution for each era\n",
    "sns.lineplot(\n",
    "    data=popularity_over_time, \n",
    "    x=\"year\", \n",
    "    y=\"track_popularity\", \n",
    "    hue=\"album_era\", \n",
    "    palette=\"coolwarm\", \n",
    "    marker=\"o\"\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel(\"Year\", fontsize=12)\n",
    "plt.ylabel(\"Average Popularity Score\", fontsize=12)\n",
    "plt.title(\"Evolution of Popularity Over Time by Song Era\", fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Original Song Era\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
