{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs with available lyrics: 72987\n",
      "                               track_uri                 album_name  \\\n",
      "4   spotify:track:5wWDfRF7NQKMx8ZPfrhBwa  Funk Volume 2013 - Single   \n",
      "10  spotify:track:1Prj89bq2VcT6YC6rNI14D                      Bravo   \n",
      "12  spotify:track:5m8amrP9cCBCeUZ1X0mTQk             All Your Fault   \n",
      "14  spotify:track:6sZHCkmQa8hzxfdI9MuywS                Sho Me Love   \n",
      "15  spotify:track:37fulLx1QJTGmxcuvEszaA    Gazing at the Moonlight   \n",
      "\n",
      "        artist_name        track_name lyrics  \n",
      "4            Hopsin  Funk Volume 2013    NaN  \n",
      "10  Jerry Purpdrank             Bravo    NaN  \n",
      "12           Hopsin    All Your Fault    NaN  \n",
      "14        Rich Gang       Sho Me Love    NaN  \n",
      "15           Hopsin     Story of Mine    NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load all Parquet files and merge them\n",
    "parquet_columns = [\"track_uri\", \"album_name\", \"artist_name\", \"track_name\"]\n",
    "parquet_files = glob.glob(\"audio_features_for_all_songs.parquet\")\n",
    "parquet_df = pd.concat([pd.read_parquet(file, columns=parquet_columns) for file in parquet_files], ignore_index=True)\n",
    "\n",
    "# Load CSV file containing song attributes and lyrics\n",
    "csv_df = pd.read_csv(\"songs_with_attributes_and_lyrics.csv\", usecols=[\"id\", \"lyrics\"], dtype=str)  # Read as string to avoid type issues\n",
    "\n",
    "# Remove \"spotify:track:\" prefix from track_uri in Parquet dataset\n",
    "parquet_df[\"clean_track_id\"] = parquet_df[\"track_uri\"].str.replace(\"spotify:track:\", \"\", regex=False)\n",
    "\n",
    "# Merge Parquet dataset with lyrics from CSV\n",
    "merged_df = parquet_df.merge(csv_df, left_on=\"clean_track_id\", right_on=\"id\", how=\"left\").drop(columns=[\"clean_track_id\", \"id\"])\n",
    "\n",
    "# Keep only songs with available lyrics\n",
    "songs_with_lyrics_df = merged_df.dropna(subset=[\"lyrics\"])\n",
    "songs_without_lyrics_df = merged_df[merged_df[\"lyrics\"].isna()]\n",
    "\n",
    "# Count the number of tracks with lyrics\n",
    "num_tracks_with_lyrics = len(songs_with_lyrics_df)\n",
    "num_without_lyrics = len(songs_without_lyrics_df)\n",
    "\n",
    "print(f\"Number of songs with available lyrics: {num_tracks_with_lyrics}\")\n",
    "print(f\"Number of songs without lyrics: {num_without_lyrics}\")\n",
    "\n",
    "# Save the filtered dataset to Parquet\n",
    "output_folder = \"lyrics\"\n",
    "output_file = os.path.join(output_folder, \"songs_with_lyrics_part1.parquet\")\n",
    "\n",
    "songs_with_lyrics_df.to_parquet(output_file, engine=\"pyarrow\", compression=\"snappy\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs with lyrics: 3160\n",
      "Number of songs without lyrics: 176089\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file containing lyrics\n",
    "csv_df = pd.read_csv(\"spotify_millsongdata.csv\", usecols=[\"artist\", \"song\", \"text\"], dtype=str)\n",
    "csv_df.rename(columns={\"text\": \"lyrics\"}, inplace=True)  # Rename lyrics column for consistency\n",
    "\n",
    "# Load Parquet file (only required columns)\n",
    "filtered_parquet_df = songs_without_lyrics_df.drop(columns=[\"lyrics\"])\n",
    "\n",
    "# Create a set of (artist, song_name) tuples for both datasets\n",
    "parquet_tracks_set = set(zip(filtered_parquet_df[\"artist_name\"].str.lower(), filtered_parquet_df[\"track_name\"].str.lower()))\n",
    "csv_tracks_set = set(zip(csv_df[\"artist\"].str.lower(), csv_df[\"song\"].str.lower()))\n",
    "\n",
    "# Find common tracks based on (artist, song name)\n",
    "common_tracks = parquet_tracks_set.intersection(csv_tracks_set)\n",
    "\n",
    "# Merge Parquet dataset with lyrics based on (artist, track_name)\n",
    "merged_df = filtered_parquet_df.merge(csv_df, \n",
    "                                      left_on=[\"artist_name\", \"track_name\"], \n",
    "                                      right_on=[\"artist\", \"song\"], \n",
    "                                      how=\"left\").drop(columns=[\"artist\", \"song\"])\n",
    "\n",
    "# Split datasets\n",
    "songs_with_lyrics_df = merged_df.dropna(subset=[\"lyrics\"])   \n",
    "songs_without_lyrics_df = merged_df[merged_df[\"lyrics\"].isna()]\n",
    "\n",
    "# Count and print stats\n",
    "num_with_lyrics = len(songs_with_lyrics_df)\n",
    "num_without_lyrics = len(songs_without_lyrics_df)\n",
    "\n",
    "print(f\"Number of songs with lyrics: {num_with_lyrics}\")\n",
    "print(f\"Number of songs without lyrics: {num_without_lyrics}\")\n",
    "\n",
    "# Save datasets to Parquet\n",
    "songs_with_lyrics_df.to_parquet(os.path.join(output_folder, \"songs_with_lyrics_part2.parquet\"), engine=\"pyarrow\", compression=\"snappy\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs with lyrics: 6493\n",
      "Number of songs without lyrics: 169599\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file containing labeled lyrics (with the 'seq' column for lyrics) and rename 'seq' to 'lyrics'\n",
    "csv_df = pd.read_csv(\"labeled_lyrics_cleaned.csv\", usecols=[\"artist\", \"song\", \"seq\"], dtype=str)\n",
    "csv_df.rename(columns={\"seq\": \"lyrics\"}, inplace=True)\n",
    "\n",
    "# Load Parquet file (only required columns)\n",
    "filtered_parquet_df = songs_without_lyrics_df.drop(columns=[\"lyrics\"])\n",
    "\n",
    "# Create a set of (artist, song_name) tuples for both datasets\n",
    "parquet_tracks_set = set(zip(filtered_parquet_df[\"artist_name\"].str.lower(), filtered_parquet_df[\"track_name\"].str.lower()))\n",
    "csv_tracks_set = set(zip(csv_df[\"artist\"].str.lower(), csv_df[\"song\"].str.lower()))\n",
    "\n",
    "# Find common tracks based on (artist, song name)\n",
    "common_tracks = parquet_tracks_set.intersection(csv_tracks_set)\n",
    "\n",
    "# Merge Parquet dataset with lyrics (from the 'lyrics' column in the CSV) based on (artist, track_name)\n",
    "merged_df = filtered_parquet_df.merge(csv_df, \n",
    "                                      left_on=[\"artist_name\", \"track_name\"], \n",
    "                                      right_on=[\"artist\", \"song\"], \n",
    "                                      how=\"left\").drop(columns=[\"artist\", \"song\"])\n",
    "\n",
    "# Split datasets\n",
    "songs_with_lyrics_df = merged_df.dropna(subset=[\"lyrics\"])   # Songs that have lyrics (based on 'lyrics' column)\n",
    "songs_without_lyrics_df = merged_df[merged_df[\"lyrics\"].isna()]  # Songs that don't have lyrics\n",
    "\n",
    "# Count and print stats\n",
    "num_with_lyrics = len(songs_with_lyrics_df)\n",
    "num_without_lyrics = len(songs_without_lyrics_df)\n",
    "\n",
    "print(f\"Number of songs with lyrics: {num_with_lyrics}\")\n",
    "print(f\"Number of songs without lyrics: {num_without_lyrics}\")\n",
    "\n",
    "# Save datasets to Parquet\n",
    "songs_with_lyrics_df.to_parquet(os.path.join(output_folder, \"songs_with_lyrics_part3.parquet\"), engine=\"pyarrow\", compression=\"snappy\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs with lyrics: 3400\n",
      "Number of songs without lyrics: 166199\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file containing song attributes and lyrics\n",
    "csv_df = pd.read_csv(\"spotify_tracks.csv\", usecols=[\"uri\", \"lyrics\"], dtype=str)  \n",
    "\n",
    "# Load Parquet file (only required columns)\n",
    "filtered_parquet_df = songs_without_lyrics_df.drop(columns=[\"lyrics\"])\n",
    "\n",
    "# Merge Parquet dataset with lyrics from CSV\n",
    "merged_df = filtered_parquet_df.merge(csv_df, left_on=\"track_uri\", right_on=\"uri\", how=\"left\").drop(columns=[\"uri\"])\n",
    "\n",
    "# Keep only songs with available lyrics\n",
    "songs_with_lyrics_df = merged_df.dropna(subset=[\"lyrics\"])\n",
    "songs_without_lyrics_df = merged_df[merged_df[\"lyrics\"].isna()]\n",
    "\n",
    "# Count and print stats\n",
    "num_with_lyrics = len(songs_with_lyrics_df)\n",
    "num_without_lyrics = len(songs_without_lyrics_df)\n",
    "\n",
    "print(f\"Number of songs with lyrics: {num_with_lyrics}\")\n",
    "print(f\"Number of songs without lyrics: {num_without_lyrics}\")\n",
    "\n",
    "# Save datasets to Parquet\n",
    "songs_with_lyrics_df.to_parquet(os.path.join(output_folder, \"songs_with_lyrics_part4.parquet\"), engine=\"pyarrow\", compression=\"snappy\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "parquet_files = glob.glob(\"songs\")\n",
    "parquet_df = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n",
    "\n",
    "# Load Parquet file (only required columns)\n",
    "filtered_parquet_df = pd.read_parquet(\"songs_without_lyrics.parquet\")\n",
    "\n",
    "# Create a set of (artist, song_name) tuples for both datasets\n",
    "parquet_tracks_set = set(zip(filtered_parquet_df[\"artist_name\"].str.lower(), filtered_parquet_df[\"track_name\"].str.lower()))\n",
    "csv_tracks_set = set(zip(parquet_df[\"artist\"].str.lower(), parquet_df[\"title\"].str.lower()))\n",
    "\n",
    "# Find common tracks based on (artist, song name)\n",
    "common_tracks = parquet_tracks_set.intersection(csv_tracks_set)\n",
    "\n",
    "# Merge Parquet dataset with lyrics (from the 'lyrics' column in the CSV) based on (artist, track_name)\n",
    "merged_df = filtered_parquet_df.merge(parquet_df, \n",
    "                                      left_on=[\"artist_name\", \"track_name\"], \n",
    "                                      right_on=[\"artist\", \"title\"], \n",
    "                                      how=\"left\").drop(columns=[\"artist\", \"title\"])\n",
    "\n",
    "# Split datasets\n",
    "songs_with_lyrics_df = merged_df.dropna(subset=[\"lyrics\"])   # Songs that have lyrics (based on 'lyrics' column)\n",
    "songs_without_lyrics_df = merged_df[merged_df[\"lyrics\"].isna()]  # Songs that don't have lyrics\n",
    "\n",
    "# Count and print stats\n",
    "num_with_lyrics = len(songs_with_lyrics_df)\n",
    "num_without_lyrics = len(songs_without_lyrics_df)\n",
    "\n",
    "print(f\"Number of songs with lyrics: {num_with_lyrics}\")\n",
    "print(f\"Number of songs without lyrics: {num_without_lyrics}\")\n",
    "\n",
    "# Save datasets to Parquet\n",
    "songs_with_lyrics_df.to_parquet(os.path.join(output_folder, \"songs_with_lyrics_part5.parquet\"), engine=\"pyarrow\", compression=\"snappy\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
